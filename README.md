# Transformers from Scratch

## Description

This project implements a **Transformer** model from scratch using Python and NumPy. The project includes the core Transformer implementation, a detailed Jupyter Notebook explaining the mathematical foundations of Transformers, and another notebook for training and testing the model.

## Project Structure

- `Transformers.py`: Contains the core implementation of the Transformer model.
- `Transformers_Advanced_Maths_Explanation.ipynb`: A Jupyter Notebook that provides advanced mathematical explanations of the Transformer model, including self-attention, multi-head attention, positional encoding, and backpropagation.
- `Transformers_train_test.ipynb`: A Jupyter Notebook for training and testing the Transformer on a dataset.
  
## Features

- **Custom Transformer implementation**: The model is built from scratch using NumPy without any external deep learning frameworks.
- **Mathematical explanation**: Detailed explanations of key concepts such as self-attention, multi-head attention, and backpropagation.
- **Training and Testing**: Includes a notebook for training the Transformer model on a dataset and evaluating its performance.

## Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/your-username/transformer-from-scratch.git
