{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers_model import Transformer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset simplifié\n",
    "data = [\n",
    "    ('hello', 'bonjour'),\n",
    "    ('how are you', 'comment ça va'),\n",
    "    ('good morning', 'bonjour'),\n",
    "    ('good night', 'bonne nuit'),\n",
    "    ('thank you', 'merci'),\n",
    "    ('see you later', 'à plus tard'),\n",
    "    ('yes', 'oui'),\n",
    "    ('no', 'non'),\n",
    "    ('please', 's\\'il vous plaît'),\n",
    "    ('i love you', 'je t\\'aime')\n",
    "]\n",
    "\n",
    "# Construction du vocabulaire\n",
    "def build_vocab(sentences):\n",
    "    vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "    idx = 2\n",
    "    for sentence in sentences:\n",
    "        for word in sentence.strip().split():\n",
    "            if word not in vocab:\n",
    "                vocab[word] = idx\n",
    "                idx += 1\n",
    "    return vocab\n",
    "\n",
    "input_sentences = [pair[0] for pair in data]\n",
    "target_sentences = [pair[1] for pair in data]\n",
    "\n",
    "input_vocab = build_vocab(input_sentences)\n",
    "target_vocab = build_vocab(target_sentences)\n",
    "\n",
    "input_vocab_size = len(input_vocab)\n",
    "target_vocab_size = len(target_vocab)\n",
    "max_seq_length = 5  # Limitation pour simplifier\n",
    "\n",
    "# Encodage des phrases\n",
    "def encode_sentence(sentence, vocab, max_length):\n",
    "    tokens = sentence.strip().split()\n",
    "    token_ids = [vocab.get(token, vocab['<UNK>']) for token in tokens]\n",
    "    if len(token_ids) < max_length:\n",
    "        token_ids += [vocab['<PAD>']] * (max_length - len(token_ids))\n",
    "    else:\n",
    "        token_ids = token_ids[:max_length]\n",
    "    return token_ids\n",
    "\n",
    "# Préparation des données d'entraînement\n",
    "X_train = []\n",
    "y_train_input = []\n",
    "y_train_output = []\n",
    "\n",
    "for src_sentence, tgt_sentence in data:\n",
    "    src_encoded = encode_sentence(src_sentence, input_vocab, max_seq_length)\n",
    "    tgt_encoded = encode_sentence(tgt_sentence, target_vocab, max_seq_length)\n",
    "    X_train.append(src_encoded)\n",
    "    y_train_input.append([target_vocab['<PAD>']] + tgt_encoded[:-1])\n",
    "    y_train_output.append(tgt_encoded)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train_input = np.array(y_train_input)\n",
    "y_train_output = np.array(y_train_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres du modèle\n",
    "d_model = 32\n",
    "n_heads = 4\n",
    "num_encoder_layers = 2\n",
    "num_decoder_layers = 2\n",
    "d_ff = 64\n",
    "\n",
    "# Création du modèle Transformer\n",
    "transformer = Transformer(\n",
    "    d_model=d_model,\n",
    "    n_heads=n_heads,\n",
    "    num_encoder_layers=num_encoder_layers,\n",
    "    num_decoder_layers=num_decoder_layers,\n",
    "    d_ff=d_ff,\n",
    "    input_vocab_size=input_vocab_size,\n",
    "    target_vocab_size=target_vocab_size,\n",
    "    max_seq_length=max_seq_length\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entraînement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.9673544402646352\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape (32,) doesn't match the broadcast shape (5,32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [5], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m d_output \u001b[38;5;241m=\u001b[39m d_output\u001b[38;5;241m.\u001b[39mreshape(batch_size, seq_length, vocab_size) \u001b[38;5;241m/\u001b[39m (batch_size \u001b[38;5;241m*\u001b[39m seq_length)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Rétropropagation\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Mise à jour des poids\u001b[39;00m\n\u001b[0;32m     32\u001b[0m transformer\u001b[38;5;241m.\u001b[39mstep(learning_rate)\n",
      "File \u001b[1;32mc:\\Users\\tomma\\OneDrive\\Bureau\\Mes projets\\Modèles from scratch\\Transformers\\transformers_model.py:92\u001b[0m, in \u001b[0;36mTransformer.backward\u001b[1;34m(self, d_output)\u001b[0m\n\u001b[0;32m     90\u001b[0m d_encoder_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Initialisation des gradients pour l'encodeur\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_layers):\n\u001b[1;32m---> 92\u001b[0m     d_decoder_output, d_enc_output \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_decoder_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m     d_encoder_output \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m d_enc_output  \u001b[38;5;66;03m# Accumulation des gradients\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# Rétropropagation à travers les embeddings de la cible\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tomma\\OneDrive\\Bureau\\Mes projets\\Modèles from scratch\\Transformers\\transformers_model.py:227\u001b[0m, in \u001b[0;36mDecoderLayer.backward\u001b[1;34m(self, d_output)\u001b[0m\n\u001b[0;32m    224\u001b[0m d_post_enc_dec_attn \u001b[38;5;241m=\u001b[39m d_enc_dec_attn \u001b[38;5;241m+\u001b[39m d_residual2\n\u001b[0;32m    226\u001b[0m \u001b[38;5;66;03m# Rétropropagation à travers la première normalisation\u001b[39;00m\n\u001b[1;32m--> 227\u001b[0m d_norm1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_post_enc_dec_attn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;66;03m# Rétropropagation à travers la connexion résiduelle après la self-attention\u001b[39;00m\n\u001b[0;32m    230\u001b[0m d_self_attn \u001b[38;5;241m=\u001b[39m d_norm1\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[1;32mc:\\Users\\tomma\\OneDrive\\Bureau\\Mes projets\\Modèles from scratch\\Transformers\\transformers_model.py:466\u001b[0m, in \u001b[0;36mLayerNormalization.backward\u001b[1;34m(self, d_output)\u001b[0m\n\u001b[0;32m    462\u001b[0m dx \u001b[38;5;241m=\u001b[39m (dx_norm \u001b[38;5;241m*\u001b[39m std_inv) \u001b[38;5;241m+\u001b[39m (dvar \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    463\u001b[0m                             x_mu \u001b[38;5;241m/\u001b[39m D) \u001b[38;5;241m+\u001b[39m (dmean \u001b[38;5;241m/\u001b[39m D)  \u001b[38;5;66;03m# (N, T, D)\u001b[39;00m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;66;03m# Mise à jour des gradients pour gamma et beta\u001b[39;00m\n\u001b[1;32m--> 466\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(d_output \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_norm, axis\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))  \u001b[38;5;66;03m# (D,)\u001b[39;00m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(d_output, axis\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))  \u001b[38;5;66;03m# (D,)\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dx\n",
      "\u001b[1;31mValueError\u001b[0m: non-broadcastable output operand with shape (32,) doesn't match the broadcast shape (5,32)"
     ]
    }
   ],
   "source": [
    "# Entraînement du modèle\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Réinitialisation des gradients\n",
    "    for param in transformer.parameters:\n",
    "        param.grad = np.zeros_like(param.data)\n",
    "\n",
    "    # Propagation avant\n",
    "    output = transformer.forward(X_train, y_train_input)\n",
    "\n",
    "    # Calcul de la perte (entropie croisée)\n",
    "    batch_size, seq_length, vocab_size = output.shape\n",
    "    output_flat = output.reshape(-1, vocab_size)\n",
    "    y_true_flat = y_train_output.reshape(-1)\n",
    "    loss = -np.sum(np.log(output_flat[np.arange(batch_size * seq_length), y_true_flat] + 1e-9)) / (batch_size * seq_length)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss}\")\n",
    "\n",
    "    # Calcul du gradient de la perte par rapport à la sortie\n",
    "    d_output = output.copy()\n",
    "    d_output = d_output.reshape(-1, vocab_size)\n",
    "    d_output[np.arange(batch_size * seq_length), y_true_flat] -= 1\n",
    "    d_output = d_output.reshape(batch_size, seq_length, vocab_size) / (batch_size * seq_length)\n",
    "\n",
    "    # Rétropropagation\n",
    "    transformer.backward(d_output)\n",
    "\n",
    "    # Mise à jour des poids\n",
    "    transformer.step(learning_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
